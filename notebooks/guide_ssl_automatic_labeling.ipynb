{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSL via Automatic Labeling\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)][1]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[![Download](images/download.png)][2][Download this Notebook][2]\n",
    "\n",
    "[1]:https://colab.research.google.com/github/masterfulai/masterful-docs/blob/main/notebooks/guide_ssl_automatic_labeling.ipynb\n",
    "[2]:https://docs.masterfulai.com/0.5.2/notebooks/guide_ssl_automatic_labeling.ipynb\n",
    "\n",
    "In this recipe, you'll perform simple data manipulation and utilize Masterful's stand-alone automatic labeling utility to improve your model's accuracy with SSL techniques. \n",
    "\n",
    "SSL, or Semi-Supervised Learning, means allowing your model to learn from both labeled and unlabeled data. Normally, SSL techniques require custom training loops and multiple losses. This recipe allows you to quickly implement one of the simplest forms of SSL: Automatic Labeling. \n",
    "\n",
    "Automatic labeling draws from the research lineage of self training, teacher-student training, and feature consistency. It can deliver material improvements to your model's accuracy. But it will not match the performance or reliablity of the full Masterful platform.\n",
    "\n",
    "Note that the stand alone automatic labeling utility is just that - stand alone. It sits apart from the rest of the Masterful platform, and you will not see it used in conjection with the rest of the API. \n",
    "\n",
    "Consider using this recipe if:\n",
    "\n",
    "* You want to quickly try an SSL technique.\n",
    "* You want to keep your own training loop. \n",
    "* You have a very well-tuned regularization policy. \n",
    "\n",
    "For power users, you may want to skip this recipe and go straight to the full Masterful CLI Trainer or Masterful Python API if:\n",
    "\n",
    "* You want to maximize accuracy.\n",
    "* Your regularization policy is not optimally tuned. \n",
    "* Your model is detection or instance segmentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, set up a standard supervised training pipeline. \n",
    "\n",
    "This will not do any SSL yet. It should resemble most supervised training pipelines you've developed. \n",
    "\n",
    "Implement functions to:\n",
    "\n",
    "* Get your dataset (`get_labeled_datasets()`)\n",
    "* Create your model architecture (`get_model()`)\n",
    "* Augment your data (`augment_images()`)\n",
    "* Train your model (`train_model()`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 5s 1s/step - loss: 2.8249 - acc: 0.1377 - val_loss: 2.2997 - val_acc: 0.1374\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 1s 489ms/step - loss: 2.7732 - acc: 0.1205 - val_loss: 2.2985 - val_acc: 0.1352\n",
      "Epoch 3/100\n",
      "\n",
      "...\n",
      "\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 344ms/step - loss: 1.6290 - acc: 0.4312 - val_loss: 2.2970 - val_acc: 0.1282\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 347ms/step - loss: 1.6206 - acc: 0.4550 - val_loss: 2.2982 - val_acc: 0.1288\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 350ms/step - loss: 1.6054 - acc: 0.4368 - val_loss: 2.2992 - val_acc: 0.1274\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00043: early stopping\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def get_labeled_datasets(train_percentage=1):\n",
    "  \"\"\"Simple function to get cifar10 as a `tf.data.Dataset`\"\"\"\n",
    "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "  # Take the first training_percentage of the training data.\n",
    "  train_cardinality = train_percentage * 50000 // 100\n",
    "  x_train = x_train[0:train_cardinality]\n",
    "  y_train = y_train[0:train_cardinality]\n",
    "\n",
    "  # Normalize data into the range [0,1]\n",
    "  x_train = x_train.astype(\"float32\") / 255.0\n",
    "  x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "  # Convert labels to one-hot.\n",
    "  y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "  y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "  # Split test into a val and test dataset.\n",
    "  x_val = x_test[:5000]\n",
    "  y_val = y_test[:5000]\n",
    "\n",
    "  x_test = x_test[5000:]\n",
    "  y_test = y_test[5000:]\n",
    "\n",
    "  # Convert the data to tf.data.Dataset.\n",
    "  train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "  val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "  test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "  # Shuffle just the training dataset.\n",
    "  train = train.shuffle(1000)\n",
    "\n",
    "  # Batch the data. The batch size is a crucial hyperparameter to\n",
    "  # take advantage of your GPU hardware. See the guide to the\n",
    "  # optimization metalearner to find out how to learn an optimal batch size.\n",
    "  train = train.batch(256)\n",
    "  val = val.batch(256)\n",
    "  test = test.batch(256)\n",
    "\n",
    "  train = train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "  return train, val, test\n",
    "\n",
    "def get_model():\n",
    "  \"\"\"Returns a minimal convnet. \"\"\"\n",
    "  inp = tf.keras.Input((32, 32, 3))\n",
    "  x = inp\n",
    "  x = tf.keras.layers.Conv2D(16, 3, activation='relu')(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.MaxPooling2D()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(32, 3, activation='relu')(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.MaxPooling2D()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(64, 3, activation='relu')(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.MaxPooling2D()(x)\n",
    "\n",
    "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "  x = tf.keras.layers.Flatten()(x)\n",
    "  x = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "  return tf.keras.Model(inp, x)\n",
    "\n",
    "def augment_image(image):\n",
    "  \"\"\"A simple augmentation pipeline.\"\"\"\n",
    "  image = tf.image.random_brightness(image, 0.1)\n",
    "  image = tf.image.random_hue(image, 0.1)\n",
    "  image = tf.image.resize(image, size=[32,32])\n",
    "  image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "  return image\n",
    "\n",
    "def train_model(model, augmented_train, validation_data, epochs=100):\n",
    "  \"\"\"A simple training loop. \"\"\"\n",
    "\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(patience=25,\n",
    "                                                    verbose=2,\n",
    "                                                    restore_best_weights=True)\n",
    "\n",
    "  # The learning rate used by the optimizer (in this case Adam)\n",
    "  # is a crucial hyperparameter to take advantage of your GPU hardware.\n",
    "  # See the guide to the optimization metalearner to find out how to\n",
    "  # learn an optimal learning rate.\n",
    "  model.compile(\n",
    "      optimizer=tfa.optimizers.LAMB(learning_rate=0.001),\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['acc'],\n",
    "  )\n",
    "\n",
    "  model.fit(augmented_train,\n",
    "            validation_data=validation_data,\n",
    "            epochs=epochs,\n",
    "            callbacks=early_stopping)\n",
    "\n",
    "# Now use the functions you just defined to train a model start to finish.\n",
    "train, val, test = get_labeled_datasets()\n",
    "augmented_train = train.map(lambda image, label: (augment_image(image), label))\n",
    "model = get_model()\n",
    "train_model(model, augmented_train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 8ms/step - loss: 2.2876 - acc: 0.1556\n"
     ]
    }
   ],
   "source": [
    "baseline_eval_metrics = model.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now you'll improve the accuracy of your model using automatic labeling. \n",
    "\n",
    "First, set up your unlabeled data as a batched `tf.data.Dataset`. Typically, each element of a batched Dataset is a tuple of tensors: `(images, labels)`. Since unlabeled data doesn't have a label, just make each element of your batched dataset a tensor: `images`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally, the unlabeled dataset comes from images that are not yet labeled. \n",
    "# To simulate that with CIFAR10, you will use 5% of the training data, but\n",
    "# remove the labels. Be sure to use the end of the training data, not the \n",
    "# beginning, to ensure that the labeled and unlabeled sets are disjoint. \n",
    "def get_unlabeled_data(train_percentage=5):\n",
    "  \"\"\"A simple function get unlabeled CIFAR10 data.\"\"\"\n",
    "  (x_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "  # Take the training_percentage of the training data.\n",
    "  # Take it from the end of the numpy array, not the begignning, to prevent\n",
    "  # overlap with the labeled data. \n",
    "  train_cardinality = train_percentage * 50000 // 100\n",
    "  x_train = x_train[-train_cardinality:]\n",
    "\n",
    "  # Perform the same processing as the `get_labeled_data()` function.\n",
    "  x_train = x_train.astype(\"float32\") / 255.0\n",
    "  train = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "  train = train.shuffle(1000)\n",
    "\n",
    "  # Batch the data. The batch size is a crucial hyperparameter to\n",
    "  # take advantage of your GPU hardware. See the guide to the\n",
    "  # optimization metalearner to find out how to learn an optimal batch size. \n",
    "  train = train.batch(256)\n",
    "\n",
    "  return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After activating with [masterful.activate][1], call the [masterful.ssl.analyze_data_then_save_to][2] utility function. The function will ensure consistent batch sizes, a consistent right ratio of labeled to unlabeled data, and optionally allow for weighting of the labeled and unlabeled data. \n",
    "\n",
    "The function will take some time to iterate through each example from both datasets, analyze them, and save the interim results to disk.\n",
    "\n",
    "[1]:../api/api_activate.rst#masterful-activate\n",
    "[2]:../api/api_ssl.rst#masterful-ssl-analyze-data-then-save-to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Masterful version 0.4.1. This software is distributed free of\n",
      "charge for personal projects and evaluation purposes.\n",
      "See http://www.masterfulai.com/personal-and-evaluation-agreement for details.\n",
      "Sign up in the next 44 days at https://www.masterfulai.com/get-it-now\n",
      "to continue using Masterful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:02, 1030.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import masterful\n",
    "\n",
    "masterful = masterful.activate()\n",
    "\n",
    "unlabeled = get_unlabeled_data()\n",
    "\n",
    "masterful.ssl.analyze_data_then_save_to(model, \n",
    "                                        train, \n",
    "                                        unlabeled, \n",
    "                                        path='/tmp/ssl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call [masterful.ssl.load_from][1] to load the record from disk into a `tf.data.Dataset`, apply your augmentation function, and train. The record includes both (a) the labeled data and (b) the unlabeled data with automatic labels. So each epoch will take longer to run.\n",
    "\n",
    "[1]:../api/api_ssl.rst#masterful-ssl-load-from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 3s 132ms/step - loss: 3.0270 - acc: 0.0470 - val_loss: 2.3087 - val_acc: 0.0836\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 2.7617 - acc: 0.0616 - val_loss: 2.3102 - val_acc: 0.0974\n",
      "Epoch 3/100\n",
      "\n",
      "...\n",
      "\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 2.2064 - acc: 0.3250 - val_loss: 2.0833 - val_acc: 0.2986\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 2.1699 - acc: 0.3193 - val_loss: 2.0858 - val_acc: 0.2804\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 2.1651 - acc: 0.3252 - val_loss: 2.0820 - val_acc: 0.2768\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 2.1664 - acc: 0.3269 - val_loss: 2.0744 - val_acc: 0.2846\n"
     ]
    }
   ],
   "source": [
    "ssl_training_data = masterful.ssl.load_from(path='/tmp/ssl').batch(256)\n",
    "\n",
    "augmented_ssl_training_data = ssl_training_data.map(\n",
    "    lambda image, label: (augment_image(image), label))\n",
    "\n",
    "new_model = get_model()\n",
    "train_model(new_model, augmented_ssl_training_data, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your newly training model against the old one. You should see an improvement in accuracy now that you are applying SSL techniques to learn from unlabeled data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 8ms/step - loss: 2.0755 - acc: 0.2944\n",
      "Run     | Test Loss | Test Accuracy\n",
      "-----------------------------------\n",
      "baseline| 2.288     |0.1556\n",
      "ssl     | 2.076     |0.2944\n"
     ]
    }
   ],
   "source": [
    "def show_eval_metrics(baseline_metrics, ssl_metrics):\n",
    "  print('Run     | Test Loss | Test Accuracy')\n",
    "  print('-----------------------------------')\n",
    "  print(f'baseline| {baseline_metrics[0]:.4}     |{baseline_metrics[1]:.4}')\n",
    "  print(f'ssl     | {ssl_metrics[0]:.4}     |{ssl_metrics[1]:.5}')\n",
    "\n",
    "ssl_eval_metrics = new_model.evaluate(test)\n",
    "\n",
    "show_eval_metrics(baseline_eval_metrics, ssl_eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Tuning\n",
    "\n",
    "To improve the results, two hyperparameters to tune are the intensity of augmentations, and the weighting of the unlabeled data. \n",
    "\n",
    "The intensity of augmentations is generally empirically discovered by a search algorithm, such as guessing and checking or grid search. If your augmentations are suboptimally tuned, consider using the full Masterful API to manage SSL end to end.\n",
    "\n",
    "The weighting of unlabeled data is also generally empirically discovered by a search algorithm. As a rule of thumb, a 1:5 ratio of labeled to unlabeled data often works well. If you have more unlabeled data than that, you'll want to downweight each unlabeled example (and vice versa). \n",
    "\n",
    "Examples are below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 3s 121ms/step - loss: 3.3689 - acc: 0.2178 - val_loss: 2.3050 - val_acc: 0.1028\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 3.0205 - acc: 0.1672 - val_loss: 2.3071 - val_acc: 0.1024\n",
      "Epoch 3/100\n",
      "\n",
      "...\n",
      "\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 2.1763 - acc: 0.3229 - val_loss: 2.0621 - val_acc: 0.2908\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 2.1764 - acc: 0.3292 - val_loss: 2.0739 - val_acc: 0.2986\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 2.1805 - acc: 0.3320 - val_loss: 2.0633 - val_acc: 0.2906\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 2.0709 - acc: 0.2942\n",
      "\n",
      "Run     | Test Loss | Test Accuracy\n",
      "-----------------------------------\n",
      "baseline| 2.288     |0.1556\n",
      "ssl     | 2.071     |0.2942\n",
      "\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 3s 119ms/step - loss: 5.5107 - acc: 0.0489 - val_loss: 2.3053 - val_acc: 0.1000\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 5.0825 - acc: 0.0871 - val_loss: 2.3054 - val_acc: 0.1090\n",
      "Epoch 3/100\n",
      "\n",
      "...\n",
      "\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 4.1386 - acc: 0.3869 - val_loss: 2.1388 - val_acc: 0.2810\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 4.1368 - acc: 0.3770 - val_loss: 2.1407 - val_acc: 0.2842\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 4.1367 - acc: 0.3685 - val_loss: 2.1330 - val_acc: 0.2834\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 2.1373 - acc: 0.2842\n",
      "\n",
      "Run     | Test Loss | Test Accuracy\n",
      "-----------------------------------\n",
      "baseline| 2.288     |0.1556\n",
      "ssl     | 2.137     |0.2842\n",
      "\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 3s 117ms/step - loss: 1.7747 - acc: 0.0883 - val_loss: 2.3029 - val_acc: 0.1126\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 1.5988 - acc: 0.0992 - val_loss: 2.3033 - val_acc: 0.0868\n",
      "Epoch 3/100\n",
      "\n",
      "...\n",
      "\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 1.1847 - acc: 0.3145 - val_loss: 1.9927 - val_acc: 0.3098\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 1.1864 - acc: 0.3123 - val_loss: 2.0123 - val_acc: 0.3042\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 1.1803 - acc: 0.2990 - val_loss: 2.0239 - val_acc: 0.2890\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00086: early stopping\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.9777 - acc: 0.3186\n",
      "\n",
      "Run     | Test Loss | Test Accuracy\n",
      "-----------------------------------\n",
      "baseline| 2.288     |0.1556\n",
      "ssl     | 1.978     |0.3186\n"
     ]
    }
   ],
   "source": [
    "# You can quickly increase your augmentation intensity by augmenting twice.\n",
    "ssl_training_data = masterful.ssl.load_from(path='/tmp/ssl')\n",
    "augmented_ssl_training_data = ssl_training_data.map(lambda image, label: (augment_image(image), label))\n",
    "augmented_ssl_training_data = augmented_ssl_training_data.batch(256)\n",
    "\n",
    "new_model = get_model()\n",
    "train_model(new_model, augmented_ssl_training_data, val)\n",
    "\n",
    "ssl_eval_metrics = new_model.evaluate(test)\n",
    "show_eval_metrics(baseline_eval_metrics, ssl_eval_metrics)\n",
    "\n",
    "# If your unlabeled training data is 4x or less the cardinality of your labeled training data,\n",
    "# you can increase the weight of the unlabeled training data. \n",
    "ssl_training_data = masterful.ssl.load_from(path='/tmp/ssl', unlabeled_weight=2.0)\n",
    "augmented_ssl_training_data = ssl_training_data.map(lambda image, label, weight: (augment_image(image), label, weight))\n",
    "augmented_ssl_training_data = augmented_ssl_training_data.batch(256)\n",
    "\n",
    "new_model = get_model()\n",
    "train_model(new_model, augmented_ssl_training_data, val)\n",
    "\n",
    "ssl_eval_metrics = new_model.evaluate(test)\n",
    "show_eval_metrics(baseline_eval_metrics, ssl_eval_metrics)\n",
    "\n",
    "# Alternatively, if your unlabeled training data is 6x or more \n",
    "# the cardinality of your labeled training data, you can decrease the \n",
    "# weight of the unlabeled training data. \n",
    "ssl_training_data = masterful.ssl.load_from(path='/tmp/ssl', unlabeled_weight=0.5)\n",
    "augmented_ssl_training_data = ssl_training_data.map(lambda image, label, weight: (augment_image(image), label, weight))\n",
    "augmented_ssl_training_data = augmented_ssl_training_data.batch(256)\n",
    "\n",
    "new_model = get_model()\n",
    "train_model(new_model, augmented_ssl_training_data, val)\n",
    "\n",
    "ssl_eval_metrics = new_model.evaluate(test)\n",
    "show_eval_metrics(baseline_eval_metrics, ssl_eval_metrics)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
